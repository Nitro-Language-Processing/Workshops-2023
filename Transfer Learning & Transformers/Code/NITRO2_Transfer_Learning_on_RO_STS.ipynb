{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TeujBmn0XXg5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Let's build a cross encoder - Step by step guide\n",
        "\n",
        "We'll use it for the STS task. We'll use the pretrained BERT model for transfer learning on this new semantic sim task."
      ],
      "metadata": {
        "id": "f2o4qpipu2HH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a44BH7iuxSE"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers pytorch_lightning\n",
        "!wget -q https://raw.githubusercontent.com/dumitrescustefan/RO-STS/master/dataset/text-similarity/RO-STS.train.tsv\n",
        "!wget -q https://raw.githubusercontent.com/dumitrescustefan/RO-STS/master/dataset/text-similarity/RO-STS.dev.tsv\n",
        "!wget -q https://raw.githubusercontent.com/dumitrescustefan/RO-STS/master/dataset/text-similarity/RO-STS.test.tsv\n",
        "\n",
        "import logging, os, sys, json, torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import pytorch_lightning as pl\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, Trainer, TrainingArguments\n",
        "from pytorch_lightning.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "# we'll define or model name here\n",
        "transformer_model_name = \"dumitrescustefan/bert-base-romanian-cased-v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading"
      ],
      "metadata": {
        "id": "J_qZxcuXvCEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# before writting any code we're going to need our tokenizer:\n",
        "tokenizer = AutoTokenizer.from_pretrained(transformer_model_name, strip_accents=False)"
      ],
      "metadata": {
        "id": "1o_vFobN3IFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file):\n",
        "        self.tokenizer = tokenizer  # we'll need this in the __getitem__ function\n",
        "        self.instances = []\n",
        "        with open(file, \"r\", encoding=\"utf8\") as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "              parts = line.split(\"\\t\")\n",
        "              self.instances.append({\n",
        "                  \"sim\": float(parts[0])/5., \n",
        "                  \"sent\": f\"[CLS]{parts[1].strip()}[SEP]{parts[2].strip()}[SEP]\"\n",
        "                  })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instances)  # return how many instances we have. It's a list after all\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.instances[index]"
      ],
      "metadata": {
        "id": "g8mEoGAhu_7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it's working. Load a dataset and print the first example."
      ],
      "metadata": {
        "id": "Up2FrrOrCHAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the MyDataset object with the test_data\n",
        "test_dataset = MyDataset(tokenizer, \"RO-STS.test.tsv\")\n",
        "instance = test_dataset[0]  # this calls our __getitem__(0) method\n",
        "\n",
        "# now let's print what it contains:\n",
        "for key in instance:\n",
        "  print(f\"{key}: {instance[key]}\")"
      ],
      "metadata": {
        "id": "v9XHkW97CEJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to collate the instances in a batch."
      ],
      "metadata": {
        "id": "ixO5x6JUDl-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollator(object):\n",
        "    def __init__(self, tokenizer, max_seq_len):\n",
        "        self.max_seq_len = max_seq_len  # this will be our model's maximum sequence length\n",
        "        self.tokenizer = tokenizer   # we still need our tokenizer to know that the pad token's id is\n",
        "             \n",
        "\n",
        "    def __call__(self, input_batch):\n",
        "        sims = []\n",
        "        sents = []\n",
        "\n",
        "        for instance in input_batch:\n",
        "          sims.append(instance['sim'])\n",
        "          sents.append(instance['sent'])\n",
        "\n",
        "        tokenized_batch = self.tokenizer(sents, padding=True, max_length = self.max_seq_len, truncation=True, return_tensors=\"pt\")\n",
        "        sims = torch.tensor(sims, dtype=torch.float)\n",
        "\n",
        "        return {\n",
        "            \"tokenized_batch\": tokenized_batch,\n",
        "            \"sim\": sims\n",
        "        }"
      ],
      "metadata": {
        "id": "OJ_LbCpzCEtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's test our collator\n",
        "test_dataset = MyDataset(tokenizer, \"RO-STS.train.tsv\")\n",
        "my_collator = MyCollator(tokenizer=tokenizer, max_seq_len=512)\n",
        "\n",
        "# crete a dataloader and get first batch of 3\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=3, collate_fn=my_collator)\n",
        "\n",
        "iterable_data = iter(test_dataloader)\n",
        "first_batch = next(iterable_data) # this is the output_batch from above\n",
        "for key in first_batch:\n",
        "  print(f\"{key} is a {first_batch[key]}\")"
      ],
      "metadata": {
        "id": "U2vJvl8Hv02G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model preparation\n",
        "\n",
        "We're finally here :)\n",
        "\n",
        "As we're using Pytorch Lightning to do the behind-the-scenes training, we do need to define a few functions: \n",
        "\n",
        "* ``__init__``, ``forward``\n",
        "* ``training_step``\n",
        "* ``validation_step``\n",
        "* ``configure_optimizers``\n",
        "\n",
        "As this is a single block of code, comments will be inline:\n"
      ],
      "metadata": {
        "id": "wUHxArRSvEuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(pl.LightningModule):\n",
        "    def __init__(self, model_name, lr=2e-05, model_max_length=512):\n",
        "        super().__init__()\n",
        "\n",
        "        print(\"Loading AutoModel [{}] ...\".format(model_name))\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, strip_accents=False)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        self.output_layer = torch.nn.Linear(768,1)\n",
        "        \n",
        "        self.loss_fct = torch.nn.MSELoss()   \n",
        "        \n",
        "        self.lr = lr\n",
        "      \n",
        "    def forward(self, tokenized_batch):\n",
        "        # we're just wrapping the code on the AutoModelForTokenClassification\n",
        "        # it needs the input_ids, attention_mask and labels\n",
        "\n",
        "        output = self.model(\n",
        "            input_ids=tokenized_batch['input_ids'],\n",
        "            attention_mask=tokenized_batch['attention_mask'],\n",
        "            return_dict=True\n",
        "        )\n",
        "        pooler_output = output['pooler_output']  # [batch_size, 768]\n",
        "        prediction = self.output_layer(pooler_output)  # [batch_size, 1]\n",
        "\n",
        "        return prediction.flatten()\n",
        "        \n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        tokenized_batch = batch['tokenized_batch']\n",
        "        sims = batch['sim']\n",
        "\n",
        "        prediction = self.forward(tokenized_batch)  # [batch_size, 1]\n",
        "        \n",
        "        loss = self.loss_fct(prediction, sims)\n",
        "\n",
        "        self.log(\"train_loss\", loss.detach().cpu().item(), on_step=True, on_epoch=True, prog_bar=True,)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        tokenized_batch = batch['tokenized_batch']\n",
        "        sims = batch['sim']\n",
        "\n",
        "        prediction = self.forward(tokenized_batch)  # [batch_size, seq_len, 768]\n",
        "       \n",
        "        loss = self.loss_fct(prediction, sims)\n",
        "\n",
        "        self.log(\"val_loss\", loss.detach().cpu().item(), on_step=True, on_epoch=True, prog_bar=True,)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW([p for p in self.parameters() if p.requires_grad], lr=self.lr, eps=1e-08)"
      ],
      "metadata": {
        "id": "fuiCMO5kv65w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training phase\n",
        "\n",
        "At this point we're ready to start training. When the code is ready, switch your colab to GPU, and run every cell up to this point, to have the training run on the GPU. Notice that Pytorch Lightning abstracts all the hassle of training on different devices. \n",
        "\n",
        "So, what do we need?\n",
        "\n",
        "We need the model itself (the ``TransformerModel`` object), and the trainer object which receives a few parameters detailed below. The trainer will move the data on GPU automatically, call ``train_step`` and ``train_epoch_end``, then do the same for validation, and then do backprop (internally calls Pytorch's ``.backward()``, ``optimizer_step`` and ``zero_grad`` to update the model weights. It also handles all the gritty stuff like early stopping, logging, model saving, distributed training (if you have more than 1 GPU), etc.\n"
      ],
      "metadata": {
        "id": "GC_AqbTkvGvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerModel(\n",
        "    model_name=transformer_model_name,\n",
        "    lr=2e-5,\n",
        "    model_max_length=512\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    devices=-1,  # uncomment this when training on gpus\n",
        "    accelerator=\"gpu\",  # uncomment this when training on gpus\n",
        "    max_epochs=2,  # set this to -1 when training fully \n",
        "    #limit_train_batches=10,  # comment this out when training fully\n",
        "    #limit_val_batches=5,  # comment this out when training fully\n",
        "    gradient_clip_val=1.0,\n",
        "    enable_checkpointing=False  # this disables saving the model each epoch\n",
        ")\n",
        "\n",
        "# instantiate dataloaders\n",
        "# a batch_size of 8 should work fine on 16GB GPUs\n",
        "train_dataloader = DataLoader(MyDataset(tokenizer, \"RO-STS.train.tsv\"), batch_size=8, collate_fn=my_collator, shuffle=True, pin_memory=True)\n",
        "validation_dataloader = DataLoader(MyDataset(tokenizer, \"RO-STS.dev.tsv\"), batch_size=8, collate_fn=my_collator, shuffle=False, pin_memory=True)\n",
        "\n",
        "# call this to start training\n",
        "trainer.fit(model, train_dataloader, validation_dataloader)"
      ],
      "metadata": {
        "id": "Lr7Fl8CHv5_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's use our model"
      ],
      "metadata": {
        "id": "jr5iQ_ybvIst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict (model, sent1, sent2):\n",
        "    # prepare a string with the concatenated sentences\n",
        "\n",
        "    # tokenize the sentence, don't forget to make it an 1-element list \n",
        "    \n",
        "    # run through the model, it returns a [batch_size] prediction\n",
        "    \n",
        "    return # return the value that you multiply by 5"
      ],
      "metadata": {
        "id": "KJ8HUnBHJFkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution (hidden)"
      ],
      "metadata": {
        "id": "TeujBmn0XXg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict (model, sent1, sent2):\n",
        "    concatenated_sentences = f\"[CLS]{sent1.strip()}[SEP]{sent2.strip()}[SEP]\"\n",
        "\n",
        "    tokenized_batch = model.tokenizer([concatenated_sentences], padding=True, max_length = 512, truncation=True, return_tensors=\"pt\")\n",
        "    \n",
        "    predictions = model.forward(tokenized_batch)  # returns a [batch_size, ]\n",
        "    \n",
        "    return predictions[0].item()*5.  # select the first item and multiply by 5"
      ],
      "metadata": {
        "id": "G8cINri0Xbj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "z9RmJ3ucX2Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's test our code\n",
        "model.eval()\n",
        "\n",
        "tests = {\n",
        "    (\"Ana are mere.\", \"Andrei are pere.\"),\n",
        "    (\"Ana are mere.\", \"Merg la munte pe un drum.\"),\n",
        "    (\"Filmul este foarte bun\", \"Filmul este extrem de slab.\")\n",
        "}\n",
        "\n",
        "for (s1, s2) in tests:\n",
        "  print(f\"{s1} || {s2} \\t SIM = {predict(model, s1, s2)}\")"
      ],
      "metadata": {
        "id": "EUApiUX2Nseg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tX47C7TwX7it"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}